{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42b5a73",
   "metadata": {},
   "source": [
    "# ü§ñ Multilayer Perceptron\n",
    "\n",
    "Esta Jupyter Notebook implementa una MLP para detectar patrones en una matriz 10x10.\n",
    "Se genera un dataset para luego entrenar el modelo y validarlo.\n",
    "Se eval√∫a la precisi√≥n para reiterar sobre el modelo con el objetivo de mejorar su eficacia.\n",
    "Finalmente se exporta el modelo para ser utilizado en la aplicaci√≥n web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941e0c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## üìã Situaci√≥n\n",
    "\n",
    "En una matriz 10x10 se debe poder detectar las letras `b`, `d`, `f` que se corresponden a los siguientes patrones:\n",
    "\n",
    "![Patrones que el MLP deber√° reconocer](assets/patterns.png)\n",
    "\n",
    "Ante un dato nuevo, el MLP deber√° ser capaz de clasificar el contenido de esa matriz en uno de los tres patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cf6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, Dict\n",
    "\n",
    "# Extend maximum width when printing DataFrames so they fit in just one line\n",
    "pd.options.display.width = 100\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 7\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "# fmt: off\n",
    "# Define the 10x10 patterns for 'b', 'd', and 'f' as a 1D array\n",
    "PATTERNS = {\n",
    "    \"b\": np.array(\n",
    "           [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"d\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"f\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64b1e6",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Generaci√≥n de Datasets\n",
    "\n",
    "Conviene generar el conjunto de datos de manera program√°tica.\n",
    "Los datasets deber√°n ser representativos a la hora de definir la distribuci√≥n de los ejemplos de entrenamiento.\n",
    "Se definen los patrones 'b', 'd', y 'f'.\n",
    "Luego, se crea una funci√≥n `generate_sample` que crea un ejemplo de un patr√≥n dado con una distorsi√≥n entre 0 y 0.3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45330a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 'b' pattern with zero noise:\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "A 'b' with 30% noise, meaning 30 out of 100 cells have been randomly flipped:\n",
      "\u001b[1;92m1\u001b[0m 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "\u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m\n",
      "\u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0\n",
      "\u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m\n",
      "0 0 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(pattern: Literal[\"b\", \"d\", \"f\"], noise: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a 1D array based on a given pattern letter ('b', 'd', or 'f'), with optional noise.\n",
    "\n",
    "    Args:\n",
    "        pattern: One of 'b', 'd', or 'f'.\n",
    "        noise: Proportion of pixels to flip.\n",
    "    Returns:\n",
    "        A numpy 1D array representing the 10x10 matrix with the pattern and noise applied.\n",
    "    \"\"\"\n",
    "    sample = PATTERNS[pattern].copy()\n",
    "    num_pixels = sample.size\n",
    "    num_noisy = int(noise * num_pixels)\n",
    "\n",
    "    if num_noisy > 0:\n",
    "        # Choose random indices to flip\n",
    "        indices = RNG.choice(num_pixels, num_noisy, replace=False)\n",
    "        # Flip the selected pixels (0 becomes 1, 1 becomes 0)\n",
    "        sample[indices] = 1 - sample[indices]\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def print_sample(sample: np.ndarray):\n",
    "    \"\"\"\n",
    "    Pretty-prints the provided 1D array as a 10x10 matrix,\n",
    "    coloring filled pixels bold green and adding an outline to the matrix.\n",
    "\n",
    "    Args:\n",
    "        sample: 1D numpy array of 0s and 1s.\n",
    "    \"\"\"\n",
    "    BOLD_GREEN = \"\\033[1;92m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    for i in range(10):\n",
    "        row = sample[i * 10 : (i + 1) * 10]\n",
    "        print(\" \".join([f\"{BOLD_GREEN}1{RESET}\" if val else \"0\" for val in row]))\n",
    "\n",
    "\n",
    "print(\"A 'b' pattern with zero noise:\")\n",
    "print_sample(generate_sample(\"d\", 0.00))\n",
    "print()\n",
    "print(\"A 'b' with 30% noise, meaning 30 out of 100 cells have been randomly flipped:\")\n",
    "print_sample(generate_sample(\"d\", 0.30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38b2bb",
   "metadata": {},
   "source": [
    "Luego se generan 3 datasets que contengan 100, 500 y 1000 ejemplos. \n",
    "El 10% ser√°n patrones sin distorsionar y el resto con una distorsi√≥n del 1% al 30%.\n",
    "Se usar√° una **distribuci√≥n uniforme** para ubicar el 90% de ejemplos en el rango de distorsi√≥n entre 0.01 y 0.30.\n",
    "\n",
    "Cada dataset ser√° un `pd.DataFrame` de `pandas` que contiene columnas del `0` al `99` (una por cada celda de la matriz) y una columna final `class` que indica la clase del patr√≥n ('b', 'd' o 'f').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b187c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de 1000 ejemplos:\n",
      "     0  1  2  3  4  5  6  7  8  9  ...  91  92  93  94  95  96  97  98  99  class\n",
      "0    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      d\n",
      "1    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "2    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "3    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "4    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..    ...\n",
      "995  0  0  0  1  0  1  0  0  0  0  ...   0   0   0   0   0   1   0   0   0      b\n",
      "996  0  0  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   1   0   1      f\n",
      "997  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "998  0  1  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "999  1  0  0  0  0  1  1  0  0  1  ...   0   1   0   0   0   1   1   1   1      d\n",
      "\n",
      "[1000 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(n_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a dataset of pattern samples.\n",
    "    The 90% of samples will have noise between 0.01 and 0.30.\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "    Returns:\n",
    "        A dataframe with 100 columns for the flattened pattern and 1 column 'class' for the pattern class.\n",
    "    \"\"\"\n",
    "    columns = [str(i) for i in range(100)] + [\"class\"]\n",
    "    df = pd.DataFrame(0, index=np.arange(n_samples), columns=columns)\n",
    "    df = df.astype({\"class\": \"str\"})\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # 10% without distortion, 90% with distortion between 1% and 30%\n",
    "        if i < int(0.1 * n_samples):\n",
    "            noise = 0.0\n",
    "        else:\n",
    "            noise = RNG.uniform(0.01, 0.30)\n",
    "\n",
    "        # Pick a pattern at random\n",
    "        pattern = RNG.choice(list(PATTERNS.keys()))\n",
    "\n",
    "        sample = generate_sample(pattern, noise).flatten()\n",
    "        df.iloc[i, :100] = sample\n",
    "        df.loc[i, \"class\"] = pattern\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_100 = generate_dataset(100)\n",
    "df_500 = generate_dataset(500)\n",
    "df_1000 = generate_dataset(1000)\n",
    "\n",
    "print(\"Dataset de 1000 ejemplos:\")\n",
    "print(df_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4837b",
   "metadata": {},
   "source": [
    "Por cada dataset deber√°n construirse tres conjuntos de validaci√≥n con 10%, 20% y 30% de los ejemplos.\n",
    "Esto da un total de 9 pares de datasets de entrenamiento y datasets de validaci√≥n:\n",
    "\n",
    "| Dataset | Cantidad de Ejemplos | % Entrenamiento | % Validaci√≥n |\n",
    "|----|--------------------|-----------------|-------------|\n",
    "|  1 | 100                | 90%             | 10%         |\n",
    "|  2 | 100                | 80%             | 20%         |\n",
    "|  3 | 100                | 70%             | 30%         |\n",
    "|  4 | 500                | 90%             | 10%         |\n",
    "|  5 | 500                | 80%             | 20%         |\n",
    "|  6 | 500                | 70%             | 30%         |\n",
    "|  7 | 1000               | 90%             | 10%         |\n",
    "|  8 | 1000               | 80%             | 20%         |\n",
    "|  9 | 1000               | 70%             | 30%         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778a91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATIOS = [0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "def split_dataset(df: pd.DataFrame, validation_ratio: float):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df: The dataset to split.\n",
    "        validation_ratio: Proportion (between 0 and 1) of the dataset to include in the validation set.\n",
    "    Returns:\n",
    "        tuple: (training_df, validation_df).\n",
    "    \"\"\"\n",
    "    shuffled_df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    validation_size = int(len(df) * validation_ratio)\n",
    "    validation_df = shuffled_df.iloc[:validation_size].reset_index(drop=True)\n",
    "    training_df = shuffled_df.iloc[validation_size:].reset_index(drop=True)\n",
    "    return training_df, validation_df\n",
    "\n",
    "\n",
    "# Store all datasets in one dictionary\n",
    "datasets = {}\n",
    "for df_name, df in [(\"100\", df_100), (\"500\", df_500), (\"1000\", df_1000)]:\n",
    "    datasets[df_name] = {}\n",
    "    for validation_ratio in VALIDATION_RATIOS:\n",
    "        training, validation = split_dataset(df, validation_ratio)\n",
    "        key = f\"val_{int(validation_ratio * 100)}%\"\n",
    "        # Store training dataset and validation dataset for each dataset, for each validation split\n",
    "        datasets[df_name][key] = {\n",
    "            \"training\": training,\n",
    "            \"validation\": validation,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003dece",
   "metadata": {},
   "source": [
    "## üß† Implementaci√≥n del Modelo\n",
    "\n",
    "Se debe implementar el algoritmo MLP que permita, dado un dataset, parametrizar la cantidad de capas, neuronas y funciones de activaci√≥n con los que se entrenar√° la red neuronal.\n",
    "\n",
    "Requerimientos para la arquitectura del modelo:\n",
    "\n",
    "- 1 o 2 capas ocultas.\n",
    "- De 5 a 10 neuronas por capa.\n",
    "- Funciones de activaci√≥n: lineal y sigmoidal.\n",
    "- Coeficiente de aprendizaje entre 0 y 1.\n",
    "- T√©rmino momento entre 0 y 1.\n",
    "\n",
    "Para estudiar el algoritmo se utiliza de referencia la bibliograf√≠a recomendada por la c√°tedra [disponible en el campus virtual](https://frre.cvg.utn.edu.ar/pluginfile.php/105673/mod_label/intro/Perceprtr%C3%B3n-MLP.pdf):\n",
    "\n",
    "- Hilera, J. R. Martinez, V. J. (2000) Redes Neuronales Artificiales. Fundamentos, modelos y aplicaciones. Alfaomega.\n",
    "\n",
    "Tambi√©n se consulta material complementario para resolver dudas durante la implementaci√≥n:\n",
    "\n",
    "- Prince, S. J. D. (2023). Understanding deep learning. The MIT Press. [http://udlbook.com](http://udlbook.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04cb2a",
   "metadata": {},
   "source": [
    "### Arquitectura\n",
    "\n",
    "La arquitectura de esta red neuronal feedforward (MLP) consiste en:\n",
    "\n",
    "- **Capa de entrada:** 100 neuronas, una por cada celda de la matriz.\n",
    "- **Capa oculta 1:** 10 neuronas.\n",
    "- **Capa oculta 2:** 5 neuronas.\n",
    "- **Capa de salida:** 3 neuronas, una para la clasificaci√≥n de cada patr√≥n 'b', 'd', 'f'.\n",
    "\n",
    "Pr√≥ximamente los hiperpar√°metros ser√°n parametrizables, pero se utiliza esta arquitectura como base. \n",
    "Los par√°metros ser√°n inicializados utilizando la t√©cnica de Xavier Glorot, y el entrenamiento del MLP consistir√° en ajustar estos par√°metros hasta lograr resultados aceptables.\n",
    "\n",
    "![Diagrama de la arquitectura del MLP](assets/architecture.png)\n",
    "\n",
    "Esta red neuronal ser√° una funci√≥n $f[X, \\phi] = Y$ que clasifica el contenido de una matriz 10x10 en uno de tres patrones. \n",
    "El contenido de la matriz 10x10 se representa como el vector de entrada $X = [x_0, x_1, \\dots, x_{99}]^T$.\n",
    "El vector de salida $Y = [y_b, y_d, y_f]^T$ contiene la predicci√≥n del modelo para cada patr√≥n.\n",
    "El conjunto de par√°metros $\\phi = \\set{B_k, W_k}_{k=0}^K$ del modelo contiene:\n",
    "\n",
    "- El vector de _biases_ $B_k$ que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1}$.\n",
    "- Los _weights_ (pesos) $W_k$ que son aplicados a la capa $k$ y que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1}  \\times D_k$.\n",
    "\n",
    "La red neuronal se puede representar utilizando notaci√≥n matricial:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &= [x_1, x_2, \\dots, x_{100}]^T \\\\\n",
    "H_1 &= a \\left[ B_{1} + W_{1} X \\right] \\\\\n",
    "H_2 &= a \\left[ B_{2} + W_{2} H_1 \\right] \\\\\n",
    "Y &= B_3 + W_{3} H_2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "donde $a[\\bullet]$ es una funci√≥n que aplica la funci√≥n de activaci√≥n por separado a cada elemento de su vector de entrada.\n",
    "\n",
    "Esa notaci√≥n se puede expandir de la siguiente manera (Prince, 2023, p. 48):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H_1\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{10}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1\\ 100} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2\\ 100} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{10 \\ 1} & w_{10\\ 2} & \\dots & w_{10\\ 100} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{100}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "H_2\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h'_1 \\\\\n",
    "h'_2 \\\\\n",
    "\\vdots \\\\\n",
    "h'_5\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b'_{1} \\\\\n",
    "b'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b'_{5}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w'_{11} & w'_{12} & \\dots & w'_{1\\ 10} \\\\\n",
    "w'_{21} & w'_{22} & \\dots & w'_{2\\ 10} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w'_{51} & w'_{52} & \\dots & w'_{5\\ 10} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "Y\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "y_b \\\\\n",
    "y_d \\\\\n",
    "y_f\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "b''_{1} \\\\\n",
    "b''_{2} \\\\\n",
    "b''_{3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w''_{11} & w''_{12} & \\dots & w''_{14} \\\\\n",
    "w''_{21} & w''_{22} & \\dots & w''_{24} \\\\\n",
    "w''_{31} & w''_{32} & \\dots & w''_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h'_{1} \\\\\n",
    "h'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h'_{5}\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Por ejemplo, la segunda neurona de la primera capa oculta se calcula como $h_1 = a[b_1 + w_{21}x_0 + \\dots + w_{2\\ 100}x_{100}]$.\n",
    "\n",
    "A diferencia de Prince, Hilera & Martinez s√≠ agregan una funci√≥n de activaci√≥n posterior a la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815abd3e",
   "metadata": {},
   "source": [
    "### Funciones de Activaci√≥n\n",
    "\n",
    "Si bien existen m√∫ltiples funciones de activaci√≥n, el trabajo pr√°ctico requiere dos en particular:\n",
    "\n",
    "- Lineal.\n",
    "- Sigmoidal.\n",
    "\n",
    "![Funciones de activaci√≥n](assets/activation_functions.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9337ee",
   "metadata": {},
   "source": [
    "### Regla de Aprendizaje\n",
    "\n",
    "Se debe elegir la regla de aprendizaje o _loss function_ a utilizar.\n",
    "Hilera & Martinez (p. 119) presentan la regla _Least Mean Squared_ o _regla delta_:\n",
    "\n",
    "$$\n",
    "\\epsilon_k^2 = \\frac{1}{2L} \\sum_{k=1}^L (d_k - s_k)^2\n",
    "$$\n",
    "\n",
    "\n",
    "mientras que Prince (p. 62) presenta una funci√≥n _Least Squares_ similar que omite el coeficiente constante:\n",
    "\n",
    "$$\n",
    "L[\\phi] = \\sum_{i=1}^I (y_i - f[X_i, \\phi])^2\n",
    "$$\n",
    "\n",
    "Ambas consisten en comparar la salida obtenida contra la deseada para obtener el costo o p√©rdida.\n",
    "El costo se deber√≠a reducir en cada iteraci√≥n del entrenamiento del MLP a medida que se ajustan los par√°metros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50978cd",
   "metadata": {},
   "source": [
    "### Momento\n",
    "\n",
    "El _momento_ es una modificaci√≥n al algoritmo _backpropagation_ del gradiente descendiente para suavizar el progreso del algoritmo y evitar oscilaciones.\n",
    "Matem√°ticamente es un coeficiente $\\beta$ que se agrega para considerar el valor de la iteraci√≥n anterior de un par√°metro al momento de calcular su nuevo valor en la iteraci√≥n siguiente.\n",
    "\n",
    "Hilera & Martinez (p. 135) lo presentan como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{ji} (t+1) &= w_{ji}(t) + \\alpha \\ \\delta_{pj}y_{pi} + \\underbrace {\\beta (w_{ji}(t) - w_{ji} (t-1))}_\\text{T√©rmino momento} \\\\\n",
    "\\Delta w_{ji} (t+1) &= \\alpha \\ \\delta_{pj}y_{pi} + \\beta \\ \\Delta w_{ji} (t) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "mientras que Prince (p. 86) lo presenta como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_{t+1} &\\leftarrow \\beta \\cdot \\mathbf{m}_t \n",
    "  + (1 - \\beta) \\sum_{i \\in \\mathcal{B}_t} \n",
    "  \\frac{\\partial \\ell_i[\\boldsymbol{\\phi}_t]}{\\partial \\boldsymbol{\\phi}} \\\\[6pt]\n",
    "\\boldsymbol{\\phi}_{t+1} &\\leftarrow \n",
    "  \\boldsymbol{\\phi}_t - \\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c933bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patr√≥n de entrada:\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "MLP Parameters initialized:\n",
      "  W1 shape: (10, 100), b1 shape: (10,)\n",
      "  W2 shape: (5, 10), b2 shape: (5,)\n",
      "  W3 shape: (3, 5), b3 shape: (3,)\n",
      "  Total parameters: 1083\n",
      "Epoch 0: Error = 0.066237, Prediction = [0.67505667 0.40895466 0.35297125]\n",
      "Epoch 20: Error = 0.034756, Prediction = [0.75887664 0.2900568  0.25741893]\n",
      "Epoch 40: Error = 0.019102, Prediction = [0.81637117 0.21020408 0.19159537]\n",
      "Epoch 60: Error = 0.011196, Prediction = [0.85606252 0.15768858 0.14693601]\n",
      "Epoch 80: Error = 0.006992, Prediction = [0.88404335 0.12250116 0.11618888]\n",
      "\n",
      "Final: Error = 0.004710, Prediction = [0.90342563 0.09918586 0.09537136], Target: [1 0 0]\n",
      "Clasificado como: b\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multilayer Perceptron for pattern recognition.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\",\n",
    "        learning_rate: float = 0.1,\n",
    "        momentum: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MLP with specified hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            activation_type: Activation function type (\"sigmoid\" or \"linear\")\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            momentum: Momentum coefficient for parameter updates\n",
    "        \"\"\"\n",
    "        self.activation_type = activation_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.params = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Initialize weights and biases using Xavier initialization, where each\n",
    "        parameter will be drawn from a normal distribution with mean of `0` and\n",
    "        a standard deviation of `sqrt(2 / (input_layer_size + output_layer_size))`\n",
    "        to help prevent vanishing or exploding gradients.\n",
    "        \"\"\"\n",
    "        # Initialize layer 1 parameters (input -> hidden1)\n",
    "        std1 = np.sqrt(2.0 / (100 + 10))\n",
    "        W1 = RNG.normal(0, std1, (10, 100))\n",
    "        b1 = np.zeros(10)\n",
    "\n",
    "        # Initialize layer 2 parameters (hidden1 -> hidden2)\n",
    "        std2 = np.sqrt(2.0 / (10 + 5))\n",
    "        W2 = RNG.normal(0, std2, (5, 10))\n",
    "        b2 = np.zeros(5)\n",
    "\n",
    "        # Initialize output layer parameters (hidden2 -> output)\n",
    "        std3 = np.sqrt(2.0 / (5 + 3))\n",
    "        W3 = RNG.normal(0, std3, (3, 5))\n",
    "        b3 = np.zeros(3)\n",
    "\n",
    "        print(\"MLP Parameters initialized:\")\n",
    "        print(f\"  W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "        print(f\"  W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "        print(f\"  W3 shape: {W3.shape}, b3 shape: {b3.shape}\")\n",
    "        print(\n",
    "            f\"  Total parameters: {W1.size + b1.size + W2.size + b2.size + W3.size + b3.size}\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"W1\": W1,\n",
    "            \"b1\": b1,\n",
    "            \"W2\": W2,\n",
    "            \"b2\": b2,\n",
    "            \"W3\": W3,\n",
    "            \"b3\": b3,\n",
    "            \"delta_W1\": np.zeros_like(W1),\n",
    "            \"delta_b1\": np.zeros_like(b1),\n",
    "            \"delta_W2\": np.zeros_like(W2),\n",
    "            \"delta_b2\": np.zeros_like(b2),\n",
    "            \"delta_W3\": np.zeros_like(W3),\n",
    "            \"delta_b3\": np.zeros_like(b3),\n",
    "        }\n",
    "\n",
    "    def get_activation_function(self):\n",
    "        \"\"\"Return the activation function based on activation type.\"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_type == \"linear\":\n",
    "            return lambda x: x\n",
    "\n",
    "    def get_activation_derivative(self):\n",
    "        \"\"\"Return the derivative of the activation function based on activation type.\"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return lambda x: 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
    "        elif self.activation_type == \"linear\":\n",
    "            return lambda x: np.ones_like(x)\n",
    "\n",
    "    def feedforward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        See: Prince, 2023 (page 103).\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "        Returns:\n",
    "            Array with all neuron activations at each layer [X, h1, h2, y].\n",
    "        \"\"\"\n",
    "        a = self.get_activation_function()\n",
    "\n",
    "        # Forward pass from input layer to hidden layer 1\n",
    "        h1 = a(self.params[\"b1\"] + np.dot(self.params[\"W1\"], X))\n",
    "\n",
    "        # Forward pass from hidden layer 1 to hidden layer 2\n",
    "        h2 = a(self.params[\"b2\"] + np.dot(self.params[\"W2\"], h1))\n",
    "\n",
    "        # Forward pass from hidden layer 2 to output layer\n",
    "        y = a(self.params[\"b3\"] + np.dot(self.params[\"W3\"], h2))\n",
    "\n",
    "        return [X, h1, h2, y]\n",
    "\n",
    "    def calculate_loss(self, d: np.ndarray, s: np.ndarray) -> float:\n",
    "        \"\"\"Compute the least mean squared error loss. See: Hilera & Martinez, 2000 (page 119).\"\"\"\n",
    "        return (1 / (2 * len(d))) * np.sum((d - s) ** 2)\n",
    "\n",
    "    def backpropagation(self, X: np.ndarray, d: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Backpropagation to compute gradients of the loss with respect to all weights and biases.\n",
    "        See: Hilera & Martinez, 2000 (pages 133-142).\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "            y: Expected output (array with 3 values, 1 for each class).\n",
    "            activations: Activations of neurons calculated by the forward pass.\n",
    "        Returns:\n",
    "            Dictionary with updated parameters.\n",
    "        \"\"\"\n",
    "        _, h1, h2, y_pred = self.feedforward(X)\n",
    "        a_derived = self.get_activation_derivative()\n",
    "\n",
    "        # Output layer (see page 134-135)\n",
    "        delta3 = (d - y_pred) * a_derived(y_pred)\n",
    "\n",
    "        delta_W3 = (\n",
    "            self.learning_rate * np.outer(delta3, h2)\n",
    "            + self.momentum * self.params[\"delta_W3\"]\n",
    "        )\n",
    "        delta_b3 = self.learning_rate * delta3 + self.momentum * self.params[\"delta_b3\"]\n",
    "\n",
    "        # Hidden layer 2 (see page 135)\n",
    "        delta2 = np.dot(self.params[\"W3\"].T, delta3) * a_derived(h2)\n",
    "\n",
    "        delta_W2 = (\n",
    "            self.learning_rate * np.outer(delta2, h1)\n",
    "            + self.momentum * self.params[\"delta_W2\"]\n",
    "        )\n",
    "        delta_b2 = self.learning_rate * delta2 + self.momentum * self.params[\"delta_b2\"]\n",
    "\n",
    "        # Hidden layer 1 (see page 135)\n",
    "        delta1 = np.dot(self.params[\"W2\"].T, delta2) * a_derived(h1)\n",
    "\n",
    "        delta_W1 = (\n",
    "            self.learning_rate * np.outer(delta1, X)\n",
    "            + self.momentum * self.params[\"delta_W1\"]\n",
    "        )\n",
    "        delta_b1 = self.learning_rate * delta1 + self.momentum * self.params[\"delta_b1\"]\n",
    "\n",
    "        # Update parameters (see page 135)\n",
    "        new_params = self.params.copy()\n",
    "        new_params[\"W1\"] += delta_W1\n",
    "        new_params[\"b1\"] += delta_b1\n",
    "        new_params[\"W2\"] += delta_W2\n",
    "        new_params[\"b2\"] += delta_b2\n",
    "        new_params[\"W3\"] += delta_W3\n",
    "        new_params[\"b3\"] += delta_b3\n",
    "\n",
    "        # Update momentum terms (see page 135)\n",
    "        new_params[\"delta_W1\"] = delta_W1\n",
    "        new_params[\"delta_b1\"] = delta_b1\n",
    "        new_params[\"delta_W2\"] = delta_W2\n",
    "        new_params[\"delta_b2\"] = delta_b2\n",
    "        new_params[\"delta_W3\"] = delta_W3\n",
    "        new_params[\"delta_b3\"] = delta_b3\n",
    "\n",
    "        return new_params\n",
    "\n",
    "    def train(self, X: np.ndarray, d: np.ndarray, epochs: int = 100) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Train the MLP for a specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (array of 100 values).\n",
    "            d: Target output (array of 3 values).\n",
    "            epochs: Number of training epochs.\n",
    "        Returns:\n",
    "            Trained parameters.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.params = self.backpropagation(X, d)\n",
    "\n",
    "            _, _, _, y_pred = self.feedforward(X)\n",
    "            error = self.calculate_loss(d, y_pred)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}: Error = {error:.6f}, Prediction = {y_pred}\")\n",
    "\n",
    "        print(f\"\\nFinal: Error = {error:.6f}, Prediction = {y_pred}, Target: {d}\")\n",
    "\n",
    "        return self.params\n",
    "\n",
    "    def classify(self, X: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Classify the input pattern into a class.\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "        Returns:\n",
    "            \"b\", \"d\", or \"f\" according to the maximal output neuron.\n",
    "        \"\"\"\n",
    "        output = self.feedforward(X)[-1]\n",
    "        idx = int(np.argmax(output))\n",
    "        classes = [\"b\", \"d\", \"f\"]\n",
    "        return classes[idx]\n",
    "\n",
    "\n",
    "# Simple test with just one sample\n",
    "X = generate_sample(\"b\", 0.0)\n",
    "print(\"Patr√≥n de entrada:\")\n",
    "print_sample(X)\n",
    "d = np.array([1, 0, 0])\n",
    "\n",
    "mlp = MLP(\"sigmoid\", 0.1, 0.1)\n",
    "mlp.train(X, d, epochs=100)\n",
    "c = mlp.classify(X)\n",
    "print(f\"Clasificado como: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2e955",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Entrenamiento\n",
    "\n",
    "Una vez implementado el MLP con sus algoritmos de feedforward y backpropagation, se puede entrenar el modelo para ajustar sus par√°metros.\n",
    "La cuesti√≥n es entrenar el MLP sobre todo un dataset y no solo un ejemplo en particular.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42b5a73",
   "metadata": {},
   "source": [
    "# ü§ñ Multilayer Perceptron\n",
    "\n",
    "Este Jupyter Notebook implementa una MLP para detectar patrones en una matriz 10x10.\n",
    "Se genera un dataset para luego entrenar el modelo y validarlo.\n",
    "Se eval√∫a la precisi√≥n para reiterar sobre el modelo con el objetivo de mejorar su eficacia.\n",
    "Finalmente se exporta el modelo para ser utilizado en la aplicaci√≥n web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941e0c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## üìã Situaci√≥n\n",
    "\n",
    "En una matriz 10x10 se debe poder detectar las letras `b`, `d`, `f` que se corresponden a los siguientes patrones:\n",
    "\n",
    "![Patrones que el MLP deber√° reconocer](assets/patterns.png)\n",
    "\n",
    "Se definen en Python los patrones 'b', 'd', y 'f'.\n",
    "Ante un dato nuevo, el MLP debe ser capaz de clasificar el contenido de la matriz en uno de los tres patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cf6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, Dict\n",
    "\n",
    "# Extend maximum width when printing DataFrames so they fit in just one line\n",
    "pd.options.display.width = 100\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 7\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "# fmt: off\n",
    "# Define the 10x10 patterns for 'b', 'd', and 'f' as a 1D array\n",
    "PATTERNS = {\n",
    "    \"b\": np.array(\n",
    "           [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"d\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"f\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64b1e6",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Generaci√≥n de Datasets\n",
    "\n",
    "Los datasets deben ser representativos a la hora de definir la distribuci√≥n de los ejemplos de entrenamiento.\n",
    "Conviene generar el conjunto de datos de manera procedural.\n",
    "Se crea una funci√≥n `generate_sample` que crea un ejemplo de un patr√≥n dado con una distorsi√≥n entre 0 y 0.3.\n",
    "\n",
    "La distorsi√≥n alterna p√≠xeles seleccionados al azar de entre los 100 p√≠xeles de la matriz, transformando ceros en unos y viceversa.\n",
    "Una distorsi√≥n (_noise_) $n$ alterna $\\frac{n * 100}{2}$ p√≠xeles, de manera que una distorsi√≥n del $100\\%$ alterna el valor de $50$ p√≠xeles y alcanza la entrop√≠a m√°xima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45330a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 'b' sample with zero noise:\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "A 'b' sample with 30% noise, meaning 15 cells have been randomly flipped:\n",
      "\u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "\u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m\n",
      "0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "\u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "\u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m\n",
      "0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m\n",
      "0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(pattern: Literal[\"b\", \"d\", \"f\"], noise: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a 1D array based on a given pattern letter ('b', 'd', or 'f'), with optional noise.\n",
    "    A given noise (between 0 and 1) flips `noise * 100 / 2` pixels, such that a noise\n",
    "    of 1 flips half the matrix (maximal entropy).\n",
    "\n",
    "    Args:\n",
    "        pattern: One of 'b', 'd', or 'f'.\n",
    "        noise: Proportion of pixels to flip. A noise of 30% will flip 15 out of 100 pixels.\n",
    "    Returns:\n",
    "        A numpy 1D array representing the 10x10 matrix with the pattern and noise applied.\n",
    "    \"\"\"\n",
    "    sample = PATTERNS[pattern].copy()\n",
    "    num_pixels = sample.size\n",
    "    num_noisy = int(noise * num_pixels / 2)\n",
    "\n",
    "    if num_noisy > 0:\n",
    "        # Choose random indices to flip\n",
    "        indices = RNG.choice(num_pixels, num_noisy, replace=False)\n",
    "        # Flip the selected pixels (0 becomes 1, 1 becomes 0)\n",
    "        sample[indices] = 1 - sample[indices]\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def print_sample(sample: np.ndarray):\n",
    "    \"\"\"Pretty-prints the provided 1D array as a 10x10 matrix.\"\"\"\n",
    "    BOLD_GREEN = \"\\033[1;92m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    for i in range(10):\n",
    "        row = sample[i * 10 : (i + 1) * 10]\n",
    "        print(\" \".join([f\"{BOLD_GREEN}1{RESET}\" if val else \"0\" for val in row]))\n",
    "\n",
    "\n",
    "print(\"A 'b' sample with zero noise:\")\n",
    "print_sample(generate_sample(\"d\", 0.00))\n",
    "print()\n",
    "print(\"A 'b' sample with 30% noise, meaning 15 cells have been randomly flipped:\")\n",
    "print_sample(generate_sample(\"d\", 0.30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38b2bb",
   "metadata": {},
   "source": [
    "Luego se generan 3 datasets que contengan 100, 500 y 1000 ejemplos. \n",
    "El 10% son patrones sin distorsionar y el resto presentan una distorsi√≥n del 1% al 30%.\n",
    "Se usa una **distribuci√≥n uniforme** para ubicar el 90% de ejemplos en el rango de distorsi√≥n entre 0.01 y 0.30.\n",
    "\n",
    "Cada dataset es un `pd.DataFrame` de `pandas` que contiene columnas del `0` al `99` (una por cada celda de la matriz) y una columna final `class` que indica la clase del patr√≥n ('b', 'd' o 'f').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b187c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de 1000 ejemplos:\n",
      "     0  1  2  3  4  5  6  7  8  9  ...  91  92  93  94  95  96  97  98  99  class\n",
      "0    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "1    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "2    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "3    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      d\n",
      "4    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      d\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..    ...\n",
      "995  0  0  0  0  0  0  0  0  1  0  ...   0   1   0   0   0   0   0   0   0      f\n",
      "996  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "997  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "998  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "999  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      d\n",
      "\n",
      "[1000 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(n_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a dataset of pattern samples.\n",
    "    The 90% of samples will have noise between 0.01 and 0.30.\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "    Returns:\n",
    "        A dataframe with 100 columns for the pattern and 1 column 'class' for the pattern class.\n",
    "    \"\"\"\n",
    "    columns = [str(i) for i in range(100)] + [\"class\"]\n",
    "    df = pd.DataFrame(0, index=np.arange(n_samples), columns=columns)\n",
    "    df = df.astype({\"class\": \"str\"})\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # 10% without distortion, 90% with distortion between 1% and 30%\n",
    "        if i < int(0.1 * n_samples):\n",
    "            noise = 0.0\n",
    "        else:\n",
    "            noise = RNG.uniform(0.01, 0.30)\n",
    "\n",
    "        # Pick a pattern at random\n",
    "        pattern = RNG.choice(list(PATTERNS.keys()))\n",
    "\n",
    "        sample = generate_sample(pattern, noise).flatten()\n",
    "        df.iloc[i, :100] = sample\n",
    "        df.loc[i, \"class\"] = pattern\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_100 = generate_dataset(100)\n",
    "df_500 = generate_dataset(500)\n",
    "df_1000 = generate_dataset(1000)\n",
    "\n",
    "print(\"Dataset de 1000 ejemplos:\")\n",
    "print(df_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4837b",
   "metadata": {},
   "source": [
    "Por cada dataset se construyen tres conjuntos de validaci√≥n con 10%, 20% y 30% de los ejemplos, implicando conjuntos de entrenamiento del 90%, 80% y 70%.\n",
    "Eso da un total de 9 pares de datasets:\n",
    "\n",
    "| Dataset | Cantidad de Ejemplos | % Entrenamiento | % Validaci√≥n |\n",
    "|----|--------------------|-----------------|-------------|\n",
    "|  1 | 100                | 90%             | 10%         |\n",
    "|  2 | 100                | 80%             | 20%         |\n",
    "|  3 | 100                | 70%             | 30%         |\n",
    "|  4 | 500                | 90%             | 10%         |\n",
    "|  5 | 500                | 80%             | 20%         |\n",
    "|  6 | 500                | 70%             | 30%         |\n",
    "|  7 | 1000               | 90%             | 10%         |\n",
    "|  8 | 1000               | 80%             | 20%         |\n",
    "|  9 | 1000               | 70%             | 30%         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778a91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, validation_ratio: float):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df: The dataset to split.\n",
    "        validation_ratio: Proportion of the dataset to include in the validation set.\n",
    "    Returns:\n",
    "        tuple: (training_df, validation_df).\n",
    "    \"\"\"\n",
    "    shuffled_df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    validation_size = int(len(df) * validation_ratio)\n",
    "    validation_df = shuffled_df.iloc[:validation_size].reset_index(drop=True)\n",
    "    training_df = shuffled_df.iloc[validation_size:].reset_index(drop=True)\n",
    "    return training_df, validation_df\n",
    "\n",
    "\n",
    "VALIDATION_RATIOS = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Store all datasets in one dictionary\n",
    "datasets = {}\n",
    "for df_name, df in [(\"100\", df_100), (\"500\", df_500), (\"1000\", df_1000)]:\n",
    "    datasets[df_name] = {}\n",
    "    for validation_ratio in VALIDATION_RATIOS:\n",
    "        training, validation = split_dataset(df, validation_ratio)\n",
    "        key = f\"val_{int(validation_ratio * 100)}%\"\n",
    "        datasets[df_name][key] = (training, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003dece",
   "metadata": {},
   "source": [
    "## üß† Implementaci√≥n del Modelo\n",
    "\n",
    "Se debe implementar el algoritmo MLP que permita, dado un dataset, parametrizar la cantidad de capas, neuronas y funciones de activaci√≥n con los que se entrenar√° la red neuronal.\n",
    "\n",
    "Requerimientos para la arquitectura del modelo:\n",
    "\n",
    "- 1 o 2 capas ocultas.\n",
    "- De 5 a 10 neuronas por capa.\n",
    "- Funciones de activaci√≥n: lineal y sigmoidal.\n",
    "- Coeficiente de aprendizaje entre 0 y 1.\n",
    "- T√©rmino momento entre 0 y 1.\n",
    "\n",
    "Se estudia el algoritmo utilizando de referencia la bibliograf√≠a recomendada por la c√°tedra [disponible en el campus virtual](https://frre.cvg.utn.edu.ar/pluginfile.php/105673/mod_label/intro/Perceprtr%C3%B3n-MLP.pdf):\n",
    "\n",
    "- Hilera, J. R. Martinez, V. J. (2000) Redes Neuronales Artificiales. Fundamentos, modelos y aplicaciones. Alfaomega.\n",
    "\n",
    "Tambi√©n se consulta material complementario para resolver dudas durante la implementaci√≥n:\n",
    "\n",
    "- Prince, S. J. D. (2023). Understanding deep learning. The MIT Press. [http://udlbook.com](http://udlbook.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04cb2a",
   "metadata": {},
   "source": [
    "### Arquitectura\n",
    "\n",
    "La arquitectura de esta red neuronal feedforward (MLP) consiste en:\n",
    "\n",
    "- **Capa de entrada:** 100 neuronas, una por cada celda de la matriz.\n",
    "- **Capa oculta 1:** 10 neuronas.\n",
    "- **Capa oculta 2:** 5 neuronas.\n",
    "- **Capa de salida:** 3 neuronas, una para la clasificaci√≥n de cada patr√≥n 'b', 'd', 'f'.\n",
    "\n",
    "Pr√≥ximamente los hiperpar√°metros ser√°n parametrizables, pero actualmente se utiliza esta arquitectura como base. \n",
    "Los par√°metros son inicializados mediante la t√©cnica de Xavier Glorot, y el entrenamiento del MLP consiste en ajustar estos par√°metros hasta lograr una p√©rdida (_loss_) menor a cierta tolerancia.\n",
    "\n",
    "![Diagrama de la arquitectura del MLP](assets/architecture.png)\n",
    "\n",
    "Esta red neuronal es una funci√≥n $f[X, \\phi] = Y$ que clasifica el contenido de una matriz 10x10 en uno de tres patrones.\n",
    "El contenido de la matriz 10x10 se representa como el vector de entrada $X = [x_0, x_1, \\dots, x_{99}]^T$.\n",
    "El vector de salida $Y = [y_b, y_d, y_f]^T$ contiene la predicci√≥n del modelo para cada patr√≥n.\n",
    "El conjunto de par√°metros $\\phi = \\set{B_k, W_k}_{k=0}^K$ del modelo contiene:\n",
    "\n",
    "- El vector de _biases_ $B_k$ que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1}$.\n",
    "- Los pesos (_weights_) $W_k$ aplicados a la capa $k$ y que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1} \\times D_k$.\n",
    "\n",
    "La red neuronal se puede representar utilizando notaci√≥n matricial:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &= [x_1, x_2, \\dots, x_{100}]^T \\\\\n",
    "H_1 &= a \\left[ B_{1} + W_{1} X \\right] \\\\\n",
    "H_2 &= a \\left[ B_{2} + W_{2} H_1 \\right] \\\\\n",
    "Y &= B_3 + W_{3} H_2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "donde $a[\\bullet]$ es una funci√≥n que aplica la funci√≥n de activaci√≥n por separado a cada elemento de su vector de entrada.\n",
    "\n",
    "Esa notaci√≥n se puede expandir de la siguiente manera (Prince, 2023, p. 48):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H_1\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{10}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1\\ 100} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2\\ 100} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{10 \\ 1} & w_{10\\ 2} & \\dots & w_{10\\ 100} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{100}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "H_2\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h'_1 \\\\\n",
    "h'_2 \\\\\n",
    "\\vdots \\\\\n",
    "h'_5\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b'_{1} \\\\\n",
    "b'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b'_{5}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w'_{11} & w'_{12} & \\dots & w'_{1\\ 10} \\\\\n",
    "w'_{21} & w'_{22} & \\dots & w'_{2\\ 10} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w'_{51} & w'_{52} & \\dots & w'_{5\\ 10} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "Y\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "y_b \\\\\n",
    "y_d \\\\\n",
    "y_f\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "b''_{1} \\\\\n",
    "b''_{2} \\\\\n",
    "b''_{3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w''_{11} & w''_{12} & \\dots & w''_{14} \\\\\n",
    "w''_{21} & w''_{22} & \\dots & w''_{24} \\\\\n",
    "w''_{31} & w''_{32} & \\dots & w''_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h'_{1} \\\\\n",
    "h'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h'_{5}\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Por ejemplo, la activaci√≥n de la segunda neurona de la primera capa oculta se calcula como $h_1 = a[b_1 + w_{21}x_0 + \\dots + w_{2\\ 100}x_{100}]$.\n",
    "\n",
    "A diferencia de Prince, Hilera & Martinez s√≠ agregan una funci√≥n de activaci√≥n posterior a la capa de salida del MLP.\n",
    "Para mantener este trabajo alineado con la bibliograf√≠a de la c√°tedra, se prioriza seguir la implementaci√≥n de Hilera & Martinez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815abd3e",
   "metadata": {},
   "source": [
    "### Funciones de Activaci√≥n\n",
    "\n",
    "Si bien existen m√∫ltiples funciones de activaci√≥n, este trabajo pr√°ctico requiere dos en particular:\n",
    "\n",
    "- Lineal.\n",
    "- Sigmoidal.\n",
    "\n",
    "![Funciones de activaci√≥n](assets/activation_functions.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9337ee",
   "metadata": {},
   "source": [
    "### Regla de Aprendizaje\n",
    "\n",
    "Se debe elegir la regla de aprendizaje o _loss function_ a utilizar.\n",
    "Hilera & Martinez (p. 119) presentan la regla _Least Mean Squared_ o _regla delta_:\n",
    "\n",
    "$$\n",
    "\\epsilon_k^2 = \\frac{1}{2L} \\sum_{k=1}^L (d_k - s_k)^2\n",
    "$$\n",
    "\n",
    "\n",
    "mientras que Prince (p. 62) presenta una funci√≥n _Least Squares_ similar que omite el coeficiente constante:\n",
    "\n",
    "$$\n",
    "L[\\phi] = \\sum_{i=1}^I (y_i - f[X_i, \\phi])^2\n",
    "$$\n",
    "\n",
    "Ambas consisten en comparar la salida obtenida contra la deseada para obtener el costo o p√©rdida.\n",
    "El costo se deber√≠a reducir en cada iteraci√≥n del entrenamiento del MLP a medida que se ajustan los par√°metros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50978cd",
   "metadata": {},
   "source": [
    "### Momento\n",
    "\n",
    "El _momento_ es una modificaci√≥n al algoritmo _backpropagation_ del gradiente descendiente para suavizar el progreso del algoritmo y evitar oscilaciones.\n",
    "Matem√°ticamente es un coeficiente $\\beta$ que se agrega para considerar el valor de un par√°metro en la iteraci√≥n anterior al momento de calcular su nuevo valor en la iteraci√≥n siguiente.\n",
    "\n",
    "Hilera & Martinez (p. 135) lo presentan como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{ji} (t+1) &= w_{ji}(t) + \\alpha \\ \\delta_{pj}y_{pi} + \\underbrace {\\beta (w_{ji}(t) - w_{ji} (t-1))}_\\text{T√©rmino momento} \\\\\n",
    "\\Delta w_{ji} (t+1) &= \\alpha \\ \\delta_{pj}y_{pi} + \\beta \\ \\Delta w_{ji} (t) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "mientras que Prince (p. 86) lo presenta como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_{t+1} &\\leftarrow \\beta \\cdot \\mathbf{m}_t \n",
    "  + (1 - \\beta) \\sum_{i \\in \\mathcal{B}_t} \n",
    "  \\frac{\\partial \\ell_i[\\boldsymbol{\\phi}_t]}{\\partial \\boldsymbol{\\phi}} \\\\[6pt]\n",
    "\\boldsymbol{\\phi}_{t+1} &\\leftarrow \n",
    "  \\boldsymbol{\\phi}_t - \\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c933bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "\u001b[1;92m1\u001b[0m 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "0 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "Epoch 0: Loss=0.1511679072950093, Accuracy=0.0\n",
      "Epoch 20: Loss=0.07908439044883146, Accuracy=1.0\n",
      "Epoch 40: Loss=0.04120678974268039, Accuracy=1.0\n",
      "Epoch 60: Loss=0.022728818657346737, Accuracy=1.0\n",
      "Epoch 80: Loss=0.01343931028668385, Accuracy=1.0\n",
      "Epoch 100: Loss=0.008474750471591264, Accuracy=1.0\n",
      "Epoch 120: Loss=0.005646481952627597, Accuracy=1.0\n",
      "Epoch 140: Loss=0.0039390413929439325, Accuracy=1.0\n",
      "Epoch 160: Loss=0.002855046595725574, Accuracy=1.0\n",
      "Epoch 180: Loss=0.002136561386612894, Accuracy=1.0\n",
      "Epoch 200: Loss=0.00164248302779416, Accuracy=1.0\n",
      "Epoch 220: Loss=0.0012918265271248347, Accuracy=1.0\n",
      "Epoch 240: Loss=0.0010360932778477315, Accuracy=1.0\n",
      "Stopping at epoch 247: Loss change 9.990715767140522e-06 < tolerance=1e-05\n",
      "Clasificado como: f\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multilayer Perceptron for pattern recognition.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\",\n",
    "        learning_rate: float = 0.1,\n",
    "        momentum: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MLP with specified hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            activation_type: Activation function type (\"sigmoid\" or \"linear\")\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            momentum: Momentum coefficient for parameter updates\n",
    "        \"\"\"\n",
    "        self.activation_type = activation_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.params = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Initialize weights and biases using Xavier initialization, where each\n",
    "        parameter will be drawn from a normal distribution with mean of `0` and\n",
    "        a standard deviation of `sqrt(2 / (input_layer_size + output_layer_size))`\n",
    "        to help prevent vanishing or exploding gradients.\n",
    "        \"\"\"\n",
    "        # Initialize layer 1 parameters (input -> hidden1)\n",
    "        std1 = np.sqrt(2.0 / (100 + 10))\n",
    "        W1 = RNG.normal(0, std1, (10, 100))\n",
    "        b1 = np.zeros(10)\n",
    "\n",
    "        # Initialize layer 2 parameters (hidden1 -> hidden2)\n",
    "        std2 = np.sqrt(2.0 / (10 + 5))\n",
    "        W2 = RNG.normal(0, std2, (5, 10))\n",
    "        b2 = np.zeros(5)\n",
    "\n",
    "        # Initialize output layer parameters (hidden2 -> output)\n",
    "        std3 = np.sqrt(2.0 / (5 + 3))\n",
    "        W3 = RNG.normal(0, std3, (3, 5))\n",
    "        b3 = np.zeros(3)\n",
    "\n",
    "        return {\n",
    "            \"W1\": W1,\n",
    "            \"b1\": b1,\n",
    "            \"W2\": W2,\n",
    "            \"b2\": b2,\n",
    "            \"W3\": W3,\n",
    "            \"b3\": b3,\n",
    "            \"delta_W1\": np.zeros_like(W1),\n",
    "            \"delta_b1\": np.zeros_like(b1),\n",
    "            \"delta_W2\": np.zeros_like(W2),\n",
    "            \"delta_b2\": np.zeros_like(b2),\n",
    "            \"delta_W3\": np.zeros_like(W3),\n",
    "            \"delta_b3\": np.zeros_like(b3),\n",
    "        }\n",
    "\n",
    "    def get_activation_function(self):\n",
    "        \"\"\"Return the activation function based on activation type.\"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_type == \"linear\":\n",
    "            return lambda x: x\n",
    "\n",
    "    def get_activation_derivative(self):\n",
    "        \"\"\"Return the derivative of the activation function based on activation type.\"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return lambda x: 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
    "        elif self.activation_type == \"linear\":\n",
    "            return lambda x: np.ones_like(x)\n",
    "\n",
    "    def feedforward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        See: Prince, 2023 (page 103) or Hilera & Martinez, 2000 (pages 138-139).\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "        Returns:\n",
    "            Array with all neuron activations at each layer [X, h1, h2, y].\n",
    "        \"\"\"\n",
    "        a = self.get_activation_function()\n",
    "\n",
    "        # Forward pass from input layer to hidden layer 1\n",
    "        h1 = a(self.params[\"b1\"] + np.dot(self.params[\"W1\"], X))\n",
    "\n",
    "        # Forward pass from hidden layer 1 to hidden layer 2\n",
    "        h2 = a(self.params[\"b2\"] + np.dot(self.params[\"W2\"], h1))\n",
    "\n",
    "        # Forward pass from hidden layer 2 to output layer\n",
    "        y = a(self.params[\"b3\"] + np.dot(self.params[\"W3\"], h2))\n",
    "\n",
    "        return [X, h1, h2, y]\n",
    "\n",
    "    def backpropagation(self, X: np.ndarray, d: np.ndarray, activations: list) -> dict:\n",
    "        \"\"\"\n",
    "        Backpropagation. See: Hilera & Martinez, 2000 (pages 133-142).\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "            d: Target output (3 values).\n",
    "            activations: Neuron activations from forward pass [X, h1, h2, y_pred].\n",
    "        Returns:\n",
    "            Dictionary with updated parameters.\n",
    "        \"\"\"\n",
    "        _, h1, h2, y_pred = activations\n",
    "        a_derived = self.get_activation_derivative()\n",
    "\n",
    "        # Output layer (see page 134-135)\n",
    "        delta3 = (d - y_pred) * a_derived(y_pred)\n",
    "\n",
    "        delta_W3 = (\n",
    "            self.learning_rate * np.outer(delta3, h2)\n",
    "            + self.momentum * self.params[\"delta_W3\"]\n",
    "        )\n",
    "        delta_b3 = self.learning_rate * delta3 + self.momentum * self.params[\"delta_b3\"]\n",
    "\n",
    "        # Hidden layer 2 (see page 135)\n",
    "        delta2 = np.dot(self.params[\"W3\"].T, delta3) * a_derived(h2)\n",
    "\n",
    "        delta_W2 = (\n",
    "            self.learning_rate * np.outer(delta2, h1)\n",
    "            + self.momentum * self.params[\"delta_W2\"]\n",
    "        )\n",
    "        delta_b2 = self.learning_rate * delta2 + self.momentum * self.params[\"delta_b2\"]\n",
    "\n",
    "        # Hidden layer 1 (see page 135)\n",
    "        delta1 = np.dot(self.params[\"W2\"].T, delta2) * a_derived(h1)\n",
    "\n",
    "        delta_W1 = (\n",
    "            self.learning_rate * np.outer(delta1, X)\n",
    "            + self.momentum * self.params[\"delta_W1\"]\n",
    "        )\n",
    "        delta_b1 = self.learning_rate * delta1 + self.momentum * self.params[\"delta_b1\"]\n",
    "\n",
    "        # Update parameters (see page 135)\n",
    "        new_params = self.params.copy()\n",
    "        new_params[\"W1\"] += delta_W1\n",
    "        new_params[\"b1\"] += delta_b1\n",
    "        new_params[\"W2\"] += delta_W2\n",
    "        new_params[\"b2\"] += delta_b2\n",
    "        new_params[\"W3\"] += delta_W3\n",
    "        new_params[\"b3\"] += delta_b3\n",
    "\n",
    "        # Update momentum terms (see page 135)\n",
    "        new_params[\"delta_W1\"] = delta_W1\n",
    "        new_params[\"delta_b1\"] = delta_b1\n",
    "        new_params[\"delta_W2\"] = delta_W2\n",
    "        new_params[\"delta_b2\"] = delta_b2\n",
    "        new_params[\"delta_W3\"] = delta_W3\n",
    "        new_params[\"delta_b3\"] = delta_b3\n",
    "\n",
    "        return new_params\n",
    "\n",
    "    def calculate_loss(self, d: np.ndarray, s: np.ndarray) -> float:\n",
    "        \"\"\"Compute least mean squared error loss. See: Hilera & Martinez, 2000 (page 119).\"\"\"\n",
    "        return (1 / (2 * len(d))) * np.sum((d - s) ** 2)\n",
    "\n",
    "    def classify(self, X: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Classify the input pattern into a class.\n",
    "\n",
    "        Args:\n",
    "            X: Input pattern (100 values).\n",
    "        Returns:\n",
    "            \"b\", \"d\", or \"f\" according to the maximal output neuron.\n",
    "        \"\"\"\n",
    "        y_pred = self.feedforward(X)[-1]\n",
    "        idx = int(np.argmax(y_pred))\n",
    "        classes = [\"b\", \"d\", \"f\"]\n",
    "        return classes[idx]\n",
    "\n",
    "    def train_dataset(\n",
    "        self, train_data: pd.DataFrame, tolerance: float = 1e-5\n",
    "    ) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Train the MLP on a complete dataset using tolerance-based early stopping.\n",
    "\n",
    "        Args:\n",
    "            train_data: DataFrame with 100 feature columns and a 'class' column.\n",
    "            tolerance: Minimum difference between losses to continue training.\n",
    "        Returns:\n",
    "            Dictionary with training history.\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        prev_avg_loss = float(\"inf\")\n",
    "        epoch = 0\n",
    "\n",
    "        while True:\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "\n",
    "            for i in range(len(train_data)):\n",
    "                # Get the sample and its labels\n",
    "                sample = np.array(train_data.iloc[i, :100].values, dtype=np.float64)\n",
    "                true_class = train_data.iloc[i, 100]\n",
    "                d = np.array(\n",
    "                    [1, 0, 0]\n",
    "                    if true_class == \"b\"\n",
    "                    else [0, 1, 0]\n",
    "                    if true_class == \"d\"\n",
    "                    else [0, 0, 1]\n",
    "                )\n",
    "\n",
    "                # Train on the given sample\n",
    "                activations = self.feedforward(sample)\n",
    "                self.params = self.backpropagation(sample, d, activations)\n",
    "                y_pred = activations[-1]\n",
    "                loss = self.calculate_loss(d, y_pred)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Check if the prediction is correct\n",
    "                predicted = self.classify(sample)\n",
    "                if predicted == true_class:\n",
    "                    correct += 1\n",
    "\n",
    "            avg_loss = epoch_loss / len(train_data)\n",
    "            accuracy = correct / len(train_data)\n",
    "\n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(accuracy)\n",
    "\n",
    "            # Print progress\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss={avg_loss}, Accuracy={accuracy}\")\n",
    "\n",
    "            # Check for stop condition\n",
    "            loss_change = abs(avg_loss - prev_avg_loss)\n",
    "            if loss_change < tolerance:\n",
    "                print(\n",
    "                    f\"Stopping at epoch {epoch}: Loss change {loss_change} < tolerance={tolerance}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            prev_avg_loss = avg_loss\n",
    "            epoch += 1\n",
    "\n",
    "        return {\"train_losses\": train_losses, \"train_accuracies\": train_accuracies}\n",
    "\n",
    "\n",
    "# Simple test with a dataset of just one sample\n",
    "single_sample_df = generate_dataset(1)\n",
    "sample = np.array(single_sample_df.iloc[0, :100].values, dtype=np.float64)\n",
    "print_sample(sample)\n",
    "\n",
    "mlp = MLP(\"sigmoid\", 0.1, 0.1)\n",
    "mlp.train_dataset(single_sample_df, tolerance=0.00001)\n",
    "c = mlp.classify(sample)\n",
    "print(f\"Clasificado como: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2e955",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Entrenamiento\n",
    "\n",
    "Una vez implementado el MLP con sus algoritmos de feedforward y backpropagation, se puede entrenar el modelo para ajustar sus par√°metros.\n",
    "Se entrena un MLP por separado para cada uno de los 9 pares de datasets generados anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0c6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset 100:val_10% ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.12170866595242806, Accuracy=0.36666666666666664\n",
      "Epoch 20: Loss=0.042893146722334984, Accuracy=0.8444444444444444\n",
      "Epoch 40: Loss=0.0020906065708039424, Accuracy=1.0\n",
      "Epoch 60: Loss=0.0004841984560127937, Accuracy=1.0\n",
      "Epoch 80: Loss=0.00020839115122174703, Accuracy=1.0\n",
      "Epoch 100: Loss=0.0001126805446180677, Accuracy=1.0\n",
      "Epoch 120: Loss=6.906749545460502e-05, Accuracy=1.0\n",
      "Stopping at epoch 135: Loss change 9.948348651622906e-07 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 100:val_20% ---\n",
      "Epoch 0: Loss=0.12281678536793475, Accuracy=0.5375\n",
      "Epoch 20: Loss=0.022674941450657203, Accuracy=1.0\n",
      "Epoch 40: Loss=0.00832575867520702, Accuracy=1.0\n",
      "Epoch 60: Loss=0.004699801629480234, Accuracy=1.0\n",
      "Epoch 80: Loss=0.004117468242704418, Accuracy=1.0\n",
      "Epoch 100: Loss=0.0027077500537565475, Accuracy=1.0\n",
      "Epoch 120: Loss=0.00430809837791474, Accuracy=1.0\n",
      "Epoch 140: Loss=0.0024466959716716303, Accuracy=1.0\n",
      "Epoch 160: Loss=0.000730997514046462, Accuracy=1.0\n",
      "Epoch 180: Loss=0.0017341421326040908, Accuracy=1.0\n",
      "Epoch 200: Loss=0.0003094628623879593, Accuracy=1.0\n",
      "Epoch 220: Loss=0.0036094662451580995, Accuracy=1.0\n",
      "Epoch 240: Loss=0.00046757243583538165, Accuracy=1.0\n",
      "Stopping at epoch 252: Loss change 3.830832836554301e-07 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 100:val_30% ---\n",
      "Epoch 0: Loss=0.116781613964231, Accuracy=0.34285714285714286\n",
      "Epoch 20: Loss=0.041809186856022466, Accuracy=0.8857142857142857\n",
      "Epoch 40: Loss=0.005580029187429205, Accuracy=1.0\n",
      "Epoch 60: Loss=0.0014676622263651857, Accuracy=1.0\n",
      "Epoch 80: Loss=0.000565794137431515, Accuracy=1.0\n",
      "Epoch 100: Loss=0.00028352188375615475, Accuracy=1.0\n",
      "Epoch 120: Loss=0.00017015313417344024, Accuracy=1.0\n",
      "Epoch 140: Loss=0.00011342577138634672, Accuracy=1.0\n",
      "Epoch 160: Loss=8.101243514172451e-05, Accuracy=1.0\n",
      "Stopping at epoch 171: Loss change 9.988090875390925e-07 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 500:val_10% ---\n",
      "Epoch 0: Loss=0.11200339465570912, Accuracy=0.4866666666666667\n",
      "Epoch 20: Loss=0.004693208572030173, Accuracy=0.9911111111111112\n",
      "Epoch 40: Loss=0.0006991020976312722, Accuracy=1.0\n",
      "Epoch 60: Loss=0.0001943311543922817, Accuracy=1.0\n",
      "Epoch 80: Loss=0.0002102237545490831, Accuracy=1.0\n",
      "Stopping at epoch 83: Loss change 2.455961373263064e-10 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 500:val_20% ---\n",
      "Epoch 0: Loss=0.10558529309055541, Accuracy=0.5575\n",
      "Epoch 20: Loss=0.019679754023811027, Accuracy=0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80713/4245942247.py:64: RuntimeWarning: overflow encountered in exp\n",
      "  return lambda x: 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss=0.007239853035721665, Accuracy=0.9775\n",
      "Epoch 60: Loss=0.004809869662381881, Accuracy=0.9975\n",
      "Epoch 80: Loss=0.007767644490959994, Accuracy=0.9775\n",
      "Epoch 100: Loss=0.006184622795122933, Accuracy=0.985\n",
      "Epoch 120: Loss=0.006034637033399577, Accuracy=0.99\n",
      "Epoch 140: Loss=0.003998717533825069, Accuracy=0.9925\n",
      "Epoch 160: Loss=0.0048040156727417885, Accuracy=0.9925\n",
      "Epoch 180: Loss=0.0040638075039118775, Accuracy=0.995\n",
      "Epoch 200: Loss=0.0035879192963647745, Accuracy=1.0\n",
      "Epoch 220: Loss=0.002471742592872296, Accuracy=1.0\n",
      "Stopping at epoch 227: Loss change 8.305201713377022e-08 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 500:val_30% ---\n",
      "Epoch 0: Loss=0.10869461902920159, Accuracy=0.54\n",
      "Epoch 20: Loss=0.0015017259047808973, Accuracy=0.9971428571428571\n",
      "Epoch 40: Loss=0.00018154244292145215, Accuracy=1.0\n",
      "Epoch 60: Loss=6.069617088336321e-05, Accuracy=1.0\n",
      "Stopping at epoch 78: Loss change 9.884225571659932e-07 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 1000:val_10% ---\n",
      "Epoch 0: Loss=0.10028842265521544, Accuracy=0.6222222222222222\n",
      "Epoch 20: Loss=0.0035898292686555736, Accuracy=0.9944444444444445\n",
      "Stopping at epoch 26: Loss change 7.432249174909797e-07 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 1000:val_20% ---\n",
      "Epoch 0: Loss=0.10851728466819816, Accuracy=0.5125\n",
      "Epoch 20: Loss=0.0012209979368042862, Accuracy=0.99875\n",
      "Stopping at epoch 38: Loss change 2.0568790799388968e-08 < tolerance=1e-06\n",
      "\n",
      "--- Dataset 1000:val_30% ---\n",
      "Epoch 0: Loss=0.1040417014964561, Accuracy=0.5214285714285715\n",
      "Epoch 20: Loss=0.0019528181632936722, Accuracy=0.9957142857142857\n",
      "Epoch 40: Loss=0.0010195174850055901, Accuracy=0.9971428571428571\n",
      "Epoch 60: Loss=0.0006546944899105968, Accuracy=0.9985714285714286\n",
      "Epoch 80: Loss=0.00034124267205347554, Accuracy=0.9985714285714286\n",
      "Epoch 100: Loss=0.0002771350846201198, Accuracy=1.0\n",
      "Stopping at epoch 108: Loss change 8.913774689717141e-09 < tolerance=1e-06\n",
      "All 9 MLPs trained.\n"
     ]
    }
   ],
   "source": [
    "mlps = {}\n",
    "histories = {}\n",
    "\n",
    "for key in datasets:\n",
    "    for val_split in datasets[key]:\n",
    "        train_data, val_data = datasets[key][val_split]\n",
    "        print(f\"\\n--- Dataset {key}:{val_split} ---\")\n",
    "        mlp = MLP(\"sigmoid\", 0.1, 0.1)\n",
    "        history = mlp.train_dataset(train_data, tolerance=0.000001)\n",
    "        mlps[key] = mlp\n",
    "        histories[key] = history\n",
    "\n",
    "print(\"All 9 MLPs trained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

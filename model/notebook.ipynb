{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42b5a73",
   "metadata": {},
   "source": [
    "# ü§ñ Multilayer Perceptron\n",
    "\n",
    "Esta Jupyter Notebook implementa una MLP para detectar patrones en una matriz 10x10.\n",
    "Se genera un dataset para luego entrenar el modelo y validarlo.\n",
    "Se eval√∫a la precisi√≥n para reiterar sobre el modelo con el objetivo de mejorar su eficacia.\n",
    "Finalmente se exporta el modelo para ser utilizado en la aplicaci√≥n web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941e0c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## üìã Situaci√≥n\n",
    "\n",
    "En una matriz 10x10 se debe poder detectar las letras `b`, `d`, `f` que se corresponden a los siguientes patrones:\n",
    "\n",
    "![Patrones que el MLP deber√° reconocer](assets/patterns.png)\n",
    "\n",
    "Ante un dato nuevo, el MLP deber√° ser capaz de clasificar el contenido de esa matriz en uno de los tres patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cf6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, List, Dict\n",
    "\n",
    "# Extend maximum width when printing DataFrames so they fit in just one line\n",
    "pd.options.display.width = 100\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 7\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "# fmt: off\n",
    "# Define the 10x10 patterns for 'b', 'd', and 'f' as a 1D array\n",
    "PATTERNS = {\n",
    "    \"b\": np.array(\n",
    "           [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"d\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "    \"f\": np.array(\n",
    "        [\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    "}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64b1e6",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Generaci√≥n de Datasets\n",
    "\n",
    "Conviene generar el conjunto de datos de manera program√°tica.\n",
    "Los datasets deber√°n ser representativos a la hora de definir la distribuci√≥n de los ejemplos de entrenamiento.\n",
    "Se definen los patrones 'b', 'd', y 'f'.\n",
    "Luego, se crea una funci√≥n `generate_sample` que crea un ejemplo de un patr√≥n dado con una distorsi√≥n entre 0 y 0.3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45330a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 'b' pattern with zero noise:\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "A 'b' with 30% noise, meaning 30 out of 100 cells have been randomly flipped:\n",
      "\u001b[1;92m1\u001b[0m 0 0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0\n",
      "\u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m\n",
      "\u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0\n",
      "\u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 \u001b[1;92m1\u001b[0m\n",
      "0 0 0 0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(pattern: Literal[\"b\", \"d\", \"f\"], noise: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a 1D array based on a given pattern letter ('b', 'd', or 'f'), with optional noise.\n",
    "\n",
    "    Args:\n",
    "        pattern: One of 'b', 'd', or 'f'.\n",
    "        noise: Proportion of pixels to flip.\n",
    "    Returns:\n",
    "        A numpy 1D array representing the 10x10 matrix with the pattern and noise applied.\n",
    "    \"\"\"\n",
    "    sample = PATTERNS[pattern].copy()\n",
    "    num_pixels = sample.size\n",
    "    num_noisy = int(noise * num_pixels)\n",
    "\n",
    "    if num_noisy > 0:\n",
    "        # Choose random indices to flip\n",
    "        indices = RNG.choice(num_pixels, num_noisy, replace=False)\n",
    "        # Flip the selected pixels (0 becomes 1, 1 becomes 0)\n",
    "        sample[indices] = 1 - sample[indices]\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def print_sample(sample: np.ndarray):\n",
    "    \"\"\"\n",
    "    Pretty-prints the provided 1D array as a 10x10 matrix,\n",
    "    coloring filled pixels bold green and adding an outline to the matrix.\n",
    "\n",
    "    Args:\n",
    "        sample: 1D numpy array of 0s and 1s.\n",
    "    \"\"\"\n",
    "    BOLD_GREEN = \"\\033[1;92m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    for i in range(10):\n",
    "        row = sample[i * 10 : (i + 1) * 10]\n",
    "        print(\" \".join([f\"{BOLD_GREEN}1{RESET}\" if val else \"0\" for val in row]))\n",
    "\n",
    "\n",
    "print(\"A 'b' pattern with zero noise:\")\n",
    "print_sample(generate_sample(\"d\", 0.00))\n",
    "print()\n",
    "print(\"A 'b' with 30% noise, meaning 30 out of 100 cells have been randomly flipped:\")\n",
    "print_sample(generate_sample(\"d\", 0.30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38b2bb",
   "metadata": {},
   "source": [
    "Luego se generan 3 datasets que contengan 100, 500 y 1000 ejemplos. \n",
    "El 10% ser√°n patrones sin distorsionar y el resto con una distorsi√≥n del 1% al 30%.\n",
    "Se usar√° una **distribuci√≥n uniforme** para ubicar el 90% de ejemplos en el rango de distorsi√≥n entre 0.01 y 0.30.\n",
    "\n",
    "Cada dataset ser√° un `pd.DataFrame` de `pandas` que contiene columnas del `0` al `99` (una por cada celda de la matriz) y una columna final `class` que indica la clase del patr√≥n ('b', 'd' o 'f').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b187c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de 1000 ejemplos:\n",
      "     0  1  2  3  4  5  6  7  8  9  ...  91  92  93  94  95  96  97  98  99  class\n",
      "0    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      d\n",
      "1    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "2    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      f\n",
      "3    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "4    0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..    ...\n",
      "995  0  0  0  1  0  1  0  0  0  0  ...   0   0   0   0   0   1   0   0   0      b\n",
      "996  0  0  0  1  0  0  0  0  0  0  ...   0   0   0   0   0   0   1   0   1      f\n",
      "997  0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "998  0  1  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0      b\n",
      "999  1  0  0  0  0  1  1  0  0  1  ...   0   1   0   0   0   1   1   1   1      d\n",
      "\n",
      "[1000 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(n_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a dataset of pattern samples.\n",
    "    The 90% of samples will have noise between 0.01 and 0.30.\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "    Returns:\n",
    "        A dataframe with 100 columns for the flattened pattern and 1 column 'class' for the pattern class.\n",
    "    \"\"\"\n",
    "    columns = [str(i) for i in range(100)] + [\"class\"]\n",
    "    df = pd.DataFrame(0, index=np.arange(n_samples), columns=columns)\n",
    "    df = df.astype({\"class\": \"str\"})\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # 10% without distortion, 90% with distortion between 1% and 30%\n",
    "        if i < int(0.1 * n_samples):\n",
    "            noise = 0.0\n",
    "        else:\n",
    "            noise = RNG.uniform(0.01, 0.30)\n",
    "\n",
    "        # Pick a pattern at random\n",
    "        pattern = RNG.choice(list(PATTERNS.keys()))\n",
    "\n",
    "        sample = generate_sample(pattern, noise).flatten()\n",
    "        df.iloc[i, :100] = sample\n",
    "        df.loc[i, \"class\"] = pattern\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_100 = generate_dataset(100)\n",
    "df_500 = generate_dataset(500)\n",
    "df_1000 = generate_dataset(1000)\n",
    "\n",
    "print(\"Dataset de 1000 ejemplos:\")\n",
    "print(df_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4837b",
   "metadata": {},
   "source": [
    "Por cada dataset deber√°n construirse tres conjuntos de validaci√≥n con 10%, 20% y 30% de los ejemplos.\n",
    "Esto da un total de 9 pares de datasets de entrenamiento y datasets de validaci√≥n:\n",
    "\n",
    "| Dataset | Cantidad de Ejemplos | % Entrenamiento | % Validaci√≥n |\n",
    "|----|--------------------|-----------------|-------------|\n",
    "|  1 | 100                | 90%             | 10%         |\n",
    "|  2 | 100                | 80%             | 20%         |\n",
    "|  3 | 100                | 70%             | 30%         |\n",
    "|  4 | 500                | 90%             | 10%         |\n",
    "|  5 | 500                | 80%             | 20%         |\n",
    "|  6 | 500                | 70%             | 30%         |\n",
    "|  7 | 1000               | 90%             | 10%         |\n",
    "|  8 | 1000               | 80%             | 20%         |\n",
    "|  9 | 1000               | 70%             | 30%         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778a91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATIOS = [0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "def split_dataset(df: pd.DataFrame, validation_ratio: float):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df: The dataset to split.\n",
    "        validation_ratio: Proportion (between 0 and 1) of the dataset to include in the validation set.\n",
    "    Returns:\n",
    "        tuple: (training_df, validation_df).\n",
    "    \"\"\"\n",
    "    shuffled_df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    validation_size = int(len(df) * validation_ratio)\n",
    "    validation_df = shuffled_df.iloc[:validation_size].reset_index(drop=True)\n",
    "    training_df = shuffled_df.iloc[validation_size:].reset_index(drop=True)\n",
    "    return training_df, validation_df\n",
    "\n",
    "\n",
    "# Store all datasets in one dictionary\n",
    "datasets = {}\n",
    "for df_name, df in [(\"100\", df_100), (\"500\", df_500), (\"1000\", df_1000)]:\n",
    "    datasets[df_name] = {}\n",
    "    for validation_ratio in VALIDATION_RATIOS:\n",
    "        training, validation = split_dataset(df, validation_ratio)\n",
    "        key = f\"val_{int(validation_ratio * 100)}%\"\n",
    "        # Store training dataset and validation dataset for each dataset, for each validation split\n",
    "        datasets[df_name][key] = {\n",
    "            \"training\": training,\n",
    "            \"validation\": validation,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003dece",
   "metadata": {},
   "source": [
    "## üß† Implementaci√≥n del Modelo\n",
    "\n",
    "Se debe implementar el algoritmo MLP que permita, dado un dataset, parametrizar la cantidad de capas, neuronas y funciones de activaci√≥n con los que se entrenar√° la red neuronal.\n",
    "\n",
    "Requerimientos para la arquitectura del modelo:\n",
    "\n",
    "- 1 o 2 capas ocultas.\n",
    "- De 5 a 10 neuronas por capa.\n",
    "- Funciones de activaci√≥n: lineal y sigmoidal.\n",
    "- Coeficiente de aprendizaje entre 0 y 1.\n",
    "- T√©rmino momento entre 0 y 1.\n",
    "\n",
    "Para estudiar el algoritmo se utiliza de referencia la bibliograf√≠a recomendada por la c√°tedra [disponible en el campus virtual](https://frre.cvg.utn.edu.ar/pluginfile.php/105673/mod_label/intro/Perceprtr%C3%B3n-MLP.pdf):\n",
    "\n",
    "- Hilera, J. R. Martinez, V. J. (2000) Redes Neuronales Artificiales. Fundamentos, modelos y aplicaciones. Alfaomega.\n",
    "\n",
    "Tambi√©n se consulta material complementario para resolver dudas durante la implementaci√≥n:\n",
    "\n",
    "- Prince, S. J. D. (2023). Understanding deep learning. The MIT Press. [http://udlbook.com](http://udlbook.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04cb2a",
   "metadata": {},
   "source": [
    "### Arquitectura\n",
    "\n",
    "La arquitectura de esta red neuronal feedforward (MLP) consiste en:\n",
    "\n",
    "- **Capa de entrada:** 100 neuronas, una por cada celda de la matriz.\n",
    "- **Capa oculta 1:** 10 neuronas.\n",
    "- **Capa oculta 2:** 5 neuronas.\n",
    "- **Capa de salida:** 3 neuronas, una para la clasificaci√≥n de cada patr√≥n 'b', 'd', 'f'.\n",
    "\n",
    "Pr√≥ximamente los hiperpar√°metros ser√°n parametrizables, pero se utiliza esta arquitectura como base.\n",
    "\n",
    "![Diagrama de la arquitectura del MLP](assets/architecture.png)\n",
    "\n",
    "Esta red neuronal ser√° una funci√≥n $f[X, \\phi] = Y$ que clasifica el contenido de una matriz 10x10 en uno de tres patrones. \n",
    "El contenido de la matriz 10x10 se representa como el vector de entrada $X = [x_0, x_1, \\dots, x_{99}]^T$.\n",
    "El vector de salida $Y = [y_b, y_d, y_f]^T$ contiene la predicci√≥n del modelo para cada patr√≥n.\n",
    "El conjunto de par√°metros $\\phi = \\set{B_k, W_k}_{k=0}^K$ del modelo contiene:\n",
    "\n",
    "- El vector de _biases_ $B_k$ que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1}$.\n",
    "- Los _weights_ (pesos) $W_k$ que son aplicados a la capa $k$ y que contribuyen a la capa $k+1$. Es de tama√±o $D_{k+1}  \\times D_k$.\n",
    "\n",
    "La red neuronal se puede representar utilizando notaci√≥n matricial:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &= [x_1, x_2, \\dots, x_{100}]^T \\\\\n",
    "H_1 &= a \\left[ B_{1} + W_{1} X \\right] \\\\\n",
    "H_2 &= a \\left[ B_{2} + W_{2} H_1 \\right] \\\\\n",
    "Y &= B_3 + W_{3} H_2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "donde $a[\\bullet]$ es una funci√≥n que aplica la funci√≥n de activaci√≥n por separado a cada elemento de su vector de entrada (Prince, 2023).\n",
    "\n",
    "Esa notaci√≥n se puede expandir de la siguiente manera:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "H_1\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{10}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1\\ 100} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2\\ 100} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{10 \\ 1} & w_{10\\ 2} & \\dots & w_{10\\ 100} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{100}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "H_2\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "h'_1 \\\\\n",
    "h'_2 \\\\\n",
    "\\vdots \\\\\n",
    "h'_5\n",
    "\\end{bmatrix}\n",
    "= \n",
    "a\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "b'_{1} \\\\\n",
    "b'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b'_{5}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w'_{11} & w'_{12} & \\dots & w'_{1\\ 10} \\\\\n",
    "w'_{21} & w'_{22} & \\dots & w'_{2\\ 10} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w'_{51} & w'_{52} & \\dots & w'_{5\\ 10} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_{1} \\\\\n",
    "h_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h_{10}\n",
    "\\end{bmatrix}\n",
    "\\right]  \\\\\n",
    "\n",
    "Y\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "y_b \\\\\n",
    "y_d \\\\\n",
    "y_f\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "b''_{1} \\\\\n",
    "b''_{2} \\\\\n",
    "b''_{3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w''_{11} & w''_{12} & \\dots & w''_{14} \\\\\n",
    "w''_{21} & w''_{22} & \\dots & w''_{24} \\\\\n",
    "w''_{31} & w''_{32} & \\dots & w''_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h'_{1} \\\\\n",
    "h'_{2} \\\\\n",
    "\\vdots \\\\\n",
    "h'_{5}\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Por ejemplo, la segunda neurona de la primera capa oculta se calcula como $h_1 = a[b_1 + w_{21}x_0 + \\dots + w_{2\\ 100}x_{100}]$.\n",
    "\n",
    "Los par√°metros de $\\phi$ ser√°n inicializados utilizando la t√©cnica de Xavier Glorot, y el entrenamiento del MLP consistir√° en ajustar estos par√°metros hasta lograr resultados aceptables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815abd3e",
   "metadata": {},
   "source": [
    "### Funciones de Activaci√≥n\n",
    "\n",
    "Si bien existen m√∫ltiples funciones de activaci√≥n, el trabajo pr√°ctico requiere dos en particular:\n",
    "\n",
    "- Lineal.\n",
    "- Sigmoidal.\n",
    "\n",
    "![Funciones de activaci√≥n](assets/activation_functions.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9337ee",
   "metadata": {},
   "source": [
    "### Regla de Aprendizaje\n",
    "\n",
    "Se debe elegir la regla de aprendizaje o _loss function_ a utilizar.\n",
    "Hilera & Martinez presentan la regla _Least Mean Squared_ o _regla delta_:\n",
    "\n",
    "$$\n",
    "\\epsilon_k^2 = \\frac{1}{2L} \\sum_{k=1}^L (d_k - s_k)^2\n",
    "$$\n",
    "\n",
    "\n",
    "mientras que Prince presenta una funci√≥n _Least Squares_ muy similar que omite el coeficiente constante:\n",
    "\n",
    "$$\n",
    "L[\\phi] = \\sum_{i=1}^I (y_i - f[X_i, \\phi])^2\n",
    "$$\n",
    "\n",
    "Ambas consisten en comparar la salida obtenida contra la deseada para obtener el costo o p√©rdida.\n",
    "El costo se deber√≠a reducir en cada iteraci√≥n del entrenamiento del MLP a medida que se ajustan los par√°metros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50978cd",
   "metadata": {},
   "source": [
    "### Momento\n",
    "\n",
    "El _momento_ es una modificaci√≥n al algoritmo _backpropagation_ del gradiente descendiente para suavizar el progreso del algoritmo y evitar oscilaciones.\n",
    "Matem√°ticamente es un coeficiente $\\beta$ que se agrega para considerar el valor de la iteraci√≥n anterior de un par√°metro al momento de calcular su nuevo valor en la iteraci√≥n siguiente.\n",
    "\n",
    "Hilera & Martinez lo presentan como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{ji} (t+1) &= w_{ji}(t) + \\alpha \\ \\delta_{pj}y_{pi} + \\underbrace {\\beta (w_{ji}(t) - w_{ji} (t-1))}_\\text{T√©rmino momento} \\\\\n",
    "\\Delta w_{ji} (t+1) &= \\alpha \\ \\delta_{pj}y_{pi} + \\beta \\ \\Delta w_{ji} (t) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "mientras que Prince lo presenta como:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_{t+1} &\\leftarrow \\beta \\cdot \\mathbf{m}_t \n",
    "  + (1 - \\beta) \\sum_{i \\in \\mathcal{B}_t} \n",
    "  \\frac{\\partial \\ell_i[\\boldsymbol{\\phi}_t]}{\\partial \\boldsymbol{\\phi}} \\\\[6pt]\n",
    "\\boldsymbol{\\phi}_{t+1} &\\leftarrow \n",
    "  \\boldsymbol{\\phi}_t - \\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29939868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input pattern:\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m 0 0 0 0 \u001b[1;92m1\u001b[0m 0 0\n",
      "0 0 \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m \u001b[1;92m1\u001b[0m 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0\n",
      "Parameters initialized:\n",
      "  W1 shape: (10, 100), b1 shape: (10,)\n",
      "  W2 shape: (5, 10), b2 shape: (5,)\n",
      "  W3 shape: (3, 5), b3 shape: (3,)\n",
      "  Total parameters: 1083\n",
      "1. Objetivo: [1, 0, 0]. Predicci√≥n: [ 0.16544066  0.03604878 -0.40975277]. Loss: 0.28856204783418027\n",
      "2. Objetivo: [1, 0, 0]. Predicci√≥n: [ 0.49042361  0.01223818 -0.26152755]. Loss: 0.10940484309405736\n",
      "3. Objetivo: [1, 0, 0]. Predicci√≥n: [ 7.24420643e-01 -3.75384281e-05 -1.53169147e-01]. Loss: 0.03313492362284643\n",
      "4. Objetivo: [1, 0, 0]. Predicci√≥n: [ 0.85893055 -0.00357482 -0.08731056]. Loss: 0.009178834356804262\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters() -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initialize weights and biases using Xavier initialization, where each\n",
    "    parameter will be drawn from a normal distribution with mean of `0` and\n",
    "    a standard deviation of `sqrt(2 / (input_layer_size + output_layer_size))`.\n",
    "    This helps prevent vanishing or exploding gradients.\n",
    "    \"\"\"\n",
    "    # Initialize layer 1 parameters (input -> hidden1)\n",
    "    std1 = np.sqrt(2.0 / (100 + 10))\n",
    "    W1 = RNG.normal(0, std1, (10, 100))\n",
    "    b1 = np.zeros(10)\n",
    "\n",
    "    # Initialize layer 2 parameters (hidden1 -> hidden2)\n",
    "    std2 = np.sqrt(2.0 / (10 + 5))\n",
    "    W2 = RNG.normal(0, std2, (5, 10))\n",
    "    b2 = np.zeros(5)\n",
    "\n",
    "    # Initialize output layer parameters (hidden2 -> output)\n",
    "    std3 = np.sqrt(2.0 / (5 + 3))\n",
    "    W3 = RNG.normal(0, std3, (3, 5))\n",
    "    b3 = np.zeros(3)\n",
    "\n",
    "    # Initialize momentum terms\n",
    "    dW1_prev = np.zeros_like(W1)\n",
    "    db1_prev = np.zeros_like(b1)\n",
    "    dW2_prev = np.zeros_like(W2)\n",
    "    db2_prev = np.zeros_like(b2)\n",
    "    dW3_prev = np.zeros_like(W3)\n",
    "    db3_prev = np.zeros_like(b3)\n",
    "\n",
    "    print(\"Parameters initialized:\")\n",
    "    print(f\"  W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "    print(f\"  W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "    print(f\"  W3 shape: {W3.shape}, b3 shape: {b3.shape}\")\n",
    "    print(\n",
    "        f\"  Total parameters: {W1.size + b1.size + W2.size + b2.size + W3.size + b3.size}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3,\n",
    "        \"dW1_prev\": dW1_prev,\n",
    "        \"db1_prev\": db1_prev,\n",
    "        \"dW2_prev\": dW2_prev,\n",
    "        \"db2_prev\": db2_prev,\n",
    "        \"dW3_prev\": dW3_prev,\n",
    "        \"db3_prev\": db3_prev,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_activation_function(activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\"):\n",
    "    \"\"\"Return the activation function based on the activation type.\"\"\"\n",
    "    if activation_type == \"sigmoid\":\n",
    "        return lambda x: 1 / (1 + np.exp(-x))\n",
    "    elif activation_type == \"linear\":\n",
    "        return lambda x: x\n",
    "\n",
    "\n",
    "def get_activation_derivative(\n",
    "    activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\",\n",
    "):\n",
    "    \"\"\"Return the derivative of the activation function based on the activation type.\"\"\"\n",
    "    if activation_type == \"sigmoid\":\n",
    "        return lambda x: 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
    "    elif activation_type == \"linear\":\n",
    "        return lambda x: np.ones_like(x)\n",
    "\n",
    "\n",
    "def feedforward(\n",
    "    X: np.ndarray,\n",
    "    params: dict,\n",
    "    activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a feedforward pass through the network.\n",
    "\n",
    "    Args:\n",
    "        X: Input pattern (100 values).\n",
    "        params: Dictionary containing weights and biases.\n",
    "        activation_type: Name of activation function to use (\"sigmoid\" or \"linear\").\n",
    "    Returns:\n",
    "        Array with all neuron activations at each layer [X, h1, h2, y].\n",
    "    \"\"\"\n",
    "    a = get_activation_function(activation_type)\n",
    "\n",
    "    # Forward pass from input layer to hidden layer 1\n",
    "    h1 = a(params[\"b1\"] + np.dot(params[\"W1\"], X.T).T)\n",
    "\n",
    "    # Forward pass from hidden layer 1 to hidden layer 2\n",
    "    h2 = a(params[\"b2\"] + np.dot(params[\"W2\"], h1.T).T)\n",
    "\n",
    "    # Forward pass from hidden layer 2 to output layer\n",
    "    y = params[\"b3\"] + np.dot(params[\"W3\"], h2.T).T\n",
    "\n",
    "    return [X, h1, h2, y]\n",
    "\n",
    "\n",
    "def calculate_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean squared error loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth target values.\n",
    "        y_pred: Predicted values from the model.\n",
    "    Returns:\n",
    "        Mean squared error between y_true and y_pred.\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    X: np.ndarray,\n",
    "    y_target: np.ndarray,\n",
    "    params: Dict[str, np.ndarray],\n",
    "    learning_rate: float = 0.1,\n",
    "    momentum: float = 0.1,\n",
    "    activation_type: Literal[\"sigmoid\", \"linear\"] = \"sigmoid\",\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform one training step: forward pass, loss calculation and backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X: Input pattern (100 values).\n",
    "        y_target: Expected output (array with 3 values, 1 for each class).\n",
    "        params: Dictionary containing weights and biases.\n",
    "        learning_rate: Learning rate for parameter updates.\n",
    "        momentum: Momentum coefficient for parameter updates.\n",
    "        activation_type: Type of activation function.\n",
    "    Returns:\n",
    "        Dictionary containing updated weights and biases.\n",
    "    \"\"\"\n",
    "    _, h1, h2, y = feedforward(X, params, activation_type)\n",
    "\n",
    "    # Get activation derivative function\n",
    "    a_derived = get_activation_derivative(activation_type)\n",
    "\n",
    "    # Backward pass\n",
    "    dL_dz3 = 2 * (y - y_target)  # Derivative of loss function\n",
    "    dL_dW3 = np.outer(dL_dz3, h2)  # Gradient w.r.t. W3 (3,5)\n",
    "    dL_db3 = dL_dz3  # Gradient w.r.t. b3\n",
    "\n",
    "    dL_dh2 = np.dot(dL_dz3, params[\"W3\"])  # Gradient w.r.t. h2\n",
    "    dL_dz2 = dL_dh2 * a_derived(h2)  # Gradient w.r.t. pre-activation of h2\n",
    "    dL_dW2 = np.outer(dL_dz2, h1)  # Gradient w.r.t. W2 (5,10)\n",
    "    dL_db2 = dL_dz2  # Gradient w.r.t. b2\n",
    "\n",
    "    dL_dh1 = np.dot(dL_dz2, params[\"W2\"])  # Gradient w.r.t. h1\n",
    "    dL_dz1 = dL_dh1 * a_derived(h1)  # Gradient w.r.t. pre-activation of h1\n",
    "    dL_dW1 = np.outer(dL_dz1, X)  # Gradient w.r.t. W1 (10,100)\n",
    "    dL_db1 = dL_dz1  # Gradient w.r.t. b1\n",
    "\n",
    "    # Update parameters with momentum\n",
    "    dW1_update = -learning_rate * dL_dW1 + momentum * params[\"dW1_prev\"]\n",
    "    db1_update = -learning_rate * dL_db1 + momentum * params[\"db1_prev\"]\n",
    "\n",
    "    dW2_update = -learning_rate * dL_dW2 + momentum * params[\"dW2_prev\"]\n",
    "    db2_update = -learning_rate * dL_db2 + momentum * params[\"db2_prev\"]\n",
    "\n",
    "    dW3_update = -learning_rate * dL_dW3 + momentum * params[\"dW3_prev\"]\n",
    "    db3_update = -learning_rate * dL_db3 + momentum * params[\"db3_prev\"]\n",
    "\n",
    "    # Update parameters\n",
    "    new_params = params.copy()\n",
    "    new_params[\"W1\"] += dW1_update\n",
    "    new_params[\"b1\"] += db1_update\n",
    "    new_params[\"W2\"] += dW2_update\n",
    "    new_params[\"b2\"] += db2_update\n",
    "    new_params[\"W3\"] += dW3_update\n",
    "    new_params[\"b3\"] += db3_update\n",
    "\n",
    "    # Update momentum terms\n",
    "    new_params[\"dW1_prev\"] = dW1_update\n",
    "    new_params[\"db1_prev\"] = db1_update\n",
    "    new_params[\"dW2_prev\"] = dW2_update\n",
    "    new_params[\"db2_prev\"] = db2_update\n",
    "    new_params[\"dW3_prev\"] = dW3_update\n",
    "    new_params[\"db3_prev\"] = db3_update\n",
    "\n",
    "    return new_params\n",
    "\n",
    "\n",
    "# Test with a single sample\n",
    "X = generate_sample(\"b\", 0.0)\n",
    "print(\"Input pattern:\")\n",
    "print_sample(X)\n",
    "y_target = [1, 0, 0]\n",
    "\n",
    "params = initialize_parameters()\n",
    "y_pred = feedforward(X, params)[-1]\n",
    "loss = calculate_loss(y_target, y_pred)\n",
    "print(f\"1. Objetivo: {y_target}. Predicci√≥n: {y_pred}. Loss: {loss}\")\n",
    "\n",
    "params = train_step(X, y_target, params, learning_rate=0.1, momentum=0.1)\n",
    "y_pred = feedforward(X, params)[-1]\n",
    "loss = calculate_loss(y_target, y_pred)\n",
    "print(f\"2. Objetivo: {y_target}. Predicci√≥n: {y_pred}. Loss: {loss}\")\n",
    "\n",
    "params = train_step(X, y_target, params, learning_rate=0.1, momentum=0.1)\n",
    "y_pred = feedforward(X, params)[-1]\n",
    "loss = calculate_loss(y_target, y_pred)\n",
    "print(f\"3. Objetivo: {y_target}. Predicci√≥n: {y_pred}. Loss: {loss}\")\n",
    "\n",
    "params = train_step(X, y_target, params, learning_rate=0.1, momentum=0.1)\n",
    "y_pred = feedforward(X, params)[-1]\n",
    "loss = calculate_loss(y_target, y_pred)\n",
    "print(f\"4. Objetivo: {y_target}. Predicci√≥n: {y_pred}. Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
